---
title: "Lab/HW 7: visualizing temporal and spatial data"
author: "YOUR NAME"
date: "May 25, 2016"
output:
  html_document:
    toc: true
    toc_float: true
---

# Instructions

> Fill in this lab worksheet at your own pace. Knit it periodically to check that things are working the same way they are when you are working in RStudio interactively. You are welcome to ask questions, consult with others, use Google, etc. At the end of the class session, email what you have to yourself so you don't lose progress, and finish it by the last class session on June 1. You will submit this as Homework 7 on Canvas. These will be evaluated by Rebecca rather than peer reviewed.

You will want to have the following libraries loaded (you can add more in if needed):

```{r load_libraries}
library(stringr)
library(readr)
library(dplyr)
library(ggplot2)
library(ggmap)
```

# Background

> Last week we saw data from health inspections of restaurants in Seattle since 2012 and used them to practice working with character/string data and regular expressions. If you need to re-download the data, you can use the code below and change `eval=FALSE` to `eval=TRUE`:

```{r download_data, eval=FALSE}
getwd() # this is where things are going
# maybe change with setwd("C:/where_there_new/files_are_gonna_go")
download.file(url = "https://www.dropbox.com/s/lu4tom4wldh76o5/seattle_restaurant_inspections.csv?dl=0?raw=1", destfile = "seattle_restaurant_inspections.csv")
```

> Now we load in the downloaded data and use `cache=TRUE` so that we don't have to repeat this each time we re-knit:

```{r load_restaurant_data, cache=TRUE}
restaurants <- read_csv("seattle_restaurant_inspections.csv",
                        col_types = "cDcccccnnciclccic")
```

> As a reminder of what these data look like:

```{r}
str(restaurants)
```

> There are often multiple rows per `Business_ID` per `Date`, such as when an establishment is given violation points for multiple problems. The `Result` and `Score` columns will have the same values on those rows for the same restaurant and date, but details of the violation type ("red" or "blue"), violation description, and violation points will differ from row to row, with different violations on different rows. Keep this duplication in mind as you work. You will need to drop extra rows to get one row per business per date, or even one row per business. You can do this using the `dplyr` concepts we've studied like the `distinct` function, or `group_by` and `summarize` or `filter` to collapse over multiple rows.


# Preparing the data

## Whitespace

> You may remember from last time that there was extra whitespace at the beginning or end of the `Name` column. Let's clean that up using `str_trim` as in Week 8. Overwrite the `Name` column with a cleaned up version of `Name` with the whitespace trimmed off:

[YOUR CODE HERE]

## Fixing longitude

> For some reason, negative signs weren't stored with the `Longitude` variable (Seattle's longitude should be about -122 degrees, not 122 degrees). Overwrite the `Longitude` column by taking `Longitude` and multiplying it by -1.

[YOUR CODE HERE]

## Restaurants only

> There are grocery scores without seating, school cafeterias, and other less relevant businesses in the data. We only want to look at restaurants. Identify and only keep businesses whose `Description` starts with `Seating`, e.g. `"Seating 51-150 - Risk Category III"`. Call this new data frame with the rows filtered `restaurants_only`.

[YOUR CODE HERE]

## Scores over time

> Now make a data frame using `restaurants_only` called `scores_over_time` with exactly one row per `Business_ID` per inspection `Date`, with the business `Name`, its `Address` and `ZIP`, its `Longitude` and `Latitude`, and the value of `Score` on each inspection date. With data structured this way, you will be able analyze trends over time for each establishment.

[YOUR CODE HERE]


## Preparing to label bad scores

> In order to label restaurants with bad scores (say, 40 and above), you'll want to make a column called `Label_40` on `scores_over_time`. It should have the `Name` if the `Score` is greater than or equal to 40, and be blank (i.e. `""`) if the `Score` is below that. Use `mutate` and `ifelse` to make this `Label_40` column.

[YOUR CODE HERE]

## Most recent scores

> We'll also want to look at just the most recent scores for each restaurant. Make a data frame called `recent_scores` from `scores_over_time` that has one row per `Business_ID`, with the business `Name`, its `Address` and `ZIP`, `Longitude` and `Latitude`, the most recent value of `Score`, the `Date` of that score, and `Label_40`. The slides from last week pertaining to looking at the most recent inspections of coffee shops have code that might help.

[YOUR CODE HERE]

# Map-making

# Mapping the recent scores

> Now, use the `ggmap` package and the longitude and latitude information to plot the most recent inspection scores for restaurants on top of a map of Seattle. Experiment with zoom levels to get the right region bounds. Try coloring and/or sizing the points according to their most recent inspection score (bigger points = higher score). You can use [`scale_color_gradient`](http://docs.ggplot2.org/current/scale_gradient.html) to set the colors so that establishments with lower scores are white or gray, and establishments with higher scores are red, and [`scale_size`](http://docs.ggplot2.org/current/scale_size.html) to set the sizes. Play with these options and map options until you get something you think looks good.

[YOUR CODE AND PLOT HERE]

## The U District

> Now repeat the plot, but zoomed in on the U District area. Add some text labels using `Label_40` for businesses whose scores were 40 or higher on their most recent inspection. See the [`ggplot2` docs on `geom_text` and `geom_label`](http://docs.ggplot2.org/current/geom_text.html) for how you can get these to look good.

[YOUR CODE AND PLOT HERE]

## International District

> Repeat the above, but for the International District instead.

[YOUR CODE AND PLOT HERE]


# Scores over time


## Sub-sampling the data

> Now we want to look at inspection scores over time for restaurants, but there are far too many to visualize. Pick something more limited to investigate and subset the `scores_over_time` data to include somewhere between around 5 and 25 establishments. To do this, you'll want to make a vector that has just the `Business_ID` or `Name` values of restaurants of interest, and then `filter` the `scores_over_time` data based on this. Some angles you could choose for doing this subsetting:

> * Restaurants near UW or on the Ave
> * Restaurants in your ZIP code
> * Your favorite fast food chain
> * Diners
> * Coffee shops in Capitol Hill
> * A type of cuisine based on words in restaurant names (e.g. for Vietnamese, you might try a filter looking for "Viet", "Pho", "Saigon")

> The string pattern matching tools from last week could be helpful.

[YOUR CODE HERE]

## Mapping your subsample

> Make a plot, appropriately cropped, showing the locations of the restaurants you've chosen with a dot for each restaurant and text labels.

[YOUR CODE AND PLOT HERE]

## Plotting time trends

> Now make a longitudinal plot! You should use `facet_wrap` by restaurant name so that you have one panel per restaurant. The x axis should be the `Date` (reformatted using [`scale_x_date`](http://docs.ggplot2.org/current/scale_date.html) to avoid extra clutter) and the y axis should be the `Score`. Use a `geom_line` layer to show the trend in scores for each restaurant. Do you observe anything interesting about the scores for the restaurants you've chosen?

[YOUR CODE, PLOT, OBSERVATIONS HERE]

## 

```{r}
just_restaurants <- restaurants %>%
    filter(substr(Description, 1, 7) == "Seating") %>%
    mutate(Longitude = -1 * Longitude)

scores_over_time <- just_restaurants %>%
    select(Business_ID, Name, Date, Score, Longitude, Latitude) %>%
    distinct() %>%
    arrange(Business_ID, Date) %>%
    mutate(Label_40 = ifelse(Score >= 40, Name, ""))

recent_scores <- scores_over_time %>%
    group_by(Business_ID) %>%
    filter(Date == max(Date))

qmplot(Longitude, Latitude, data = recent_scores,
       maptype = "toner-lite",
       color = Score, size = Score,
       label = Label_40,
       legend = "bottomleft",
       alpha = 0.2) +
    geom_text(size = 0.5, color = "black") + 
    scale_color_gradient(low = "#888888", high = "#FF0000") +
    scale_size(guide = FALSE)


```

